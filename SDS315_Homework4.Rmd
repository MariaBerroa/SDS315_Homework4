---
title: "SDS315 Homework4"
date: "2025-02-19"
output: html_document
---

##### Name: Maria Berroa

##### UTEID: mpb2544

##### GitHub: https://github.com/MariaBerroa/SDS315_Homework4

```{r, echo=FALSE, include=FALSE}
#Import libraries and files
library(ggplot2)
library(tidyverse)
library(mosaic)
library(knitr)

letter_frequencies <- read.csv("letter_frequencies.csv")

```


#### **Problem 1 - Iron Bank**

##### We are testing SEC's null hypothesis which states that there is a 2.4% baseline probability that any "legal trade" will be flagged; whether it is from Iron Bank traders or other traders.

##### To measure evidence against the null hypothesis, we simulated 100,000 sets of 2021 "legal trades", each having a 2.4% probability of being flagged.

##### Our histogram shows that the typical number of flagged trades falls within the 45-55 range. However, we observed that Iron Bank got 70 trades flagged, which is unsually high compared to our simulated distribution. 

##### Our p-value is 0.00197, meaning that if we assume the null hypothesis is true, there would be only a 0.20% probability of observing 70 or more flagged trades just by chance. Thus the null hypothesis does not appear plausible because 70 flagged trades falls on the extreme tail of our distribution. Suggesting that Iron Bank traders are getting flagged at a rate higher than 2.4%.

```{r, echo=FALSE, results='hide'}

#This gives us the probability of how many "flags" we would get if we made 2021 fair trades

nflip(n=2021, prob=0.024)

#We will repeat these "trades" several times to see what kind of data we would get.

flag_simulation = do(100000)*nflip(n=2021, prob=0.024)

#Now we visualize the distribution of the results

ggplot(flag_simulation) + 
  geom_histogram(aes(x=nflip), binwidth=1)

#How many simulations yield 70 flags or more? 

sum(flag_simulation >= 70)

#As a proportion of the total number of simulations? 
#This is our p value

sum(flag_simulation >= 70)/100000

```


#### **Problem 2 - Health Inspections**

##### We are testing the Health Department's null hypothesis which states that, on average, 3% of restaurant inspections result in health code violations due to random issues that can occur even in well-managed establishments.

##### To measure evidence against the null hypothesis, we simulated 100,000 sets of 50 restaurant inspections, assuming that each inspection has a 3% probability of resulting in a violation.

##### Our histogram shows that the typical number of healthcode violations falls within the 1-2 range. However, Gourmet Bites was cited for healthcode violations 8 times out of the 50 inspections, which is unsually high compared to our simulated distribution. 

##### Our p-value is  0.00012, meaning that if the null hypothesis is true, there would be only a 0.012% chance of observing 8 or more inspections that result in health code violations just by chance. Thus the null hypothesis does not appear plausible because our observed results fall on the extreme tail of our distribution. Suggesting that Gourmet Bite's health code violation rate is significantly higher than the citywide average of 3%.

```{r, echo=FALSE, results='hide'}

#This gives us the probability of how many "health code violations" we would get if we conducted 50 inspections

nflip(n=50, prob=0.03)

#We will repeat these "inspections" several times to see what kind of distribution we would get.

health_violation_simulation = do(100000)*nflip(n=50, prob=0.03)

#Now we visualize the distribution of the results

ggplot(health_violation_simulation) + 
  geom_histogram(aes(x=nflip), binwidth=1)

#How many simulations yield 8 violations or more? 

sum(health_violation_simulation >= 8)

#As a proportion of the total number of simulations? 
#This is our p value

sum(health_violation_simulation >= 8)/100000

```

#### **Problem 3 - Evaluating Jury Selection for Bias**

##### The null hypthesis we are testing states that the jury selection process fairly represents the county's elegible population. Meaning the distribution of selected jurors aligns with the expected racial group proportions.

##### To measure evidence against the null hypothesis, we first established the demographic breakdown of the county's elegible jury pool (our expected values) and compared it to the observed counts of empaneled jurors across 20 trials.Then, we proceeded to conduct 100,000 simulations, each selecting 240 jurors; assuming that each selection was proportional to the county's demographics. Finally, we computed the chi square statistic for each of the 100,000 simulations, assesing the deviation between the our expected and observered juror counts proportion.

##### Our histogram shows that,on average, deviations from the expected jury distribution fall within the 1-7 range. However, the observed chi-square statistic we obtained from our observed jury selection is 12.43, which is unsually high compared to our simulated expected deviation distribution.

##### Our p-value is  0.0136, meaning that if the null hypothesis is true, there would be only a 1.36% chance of observing a deviation of 12.43 or more just by chance. Since this is an extremely low probability, the null hypothesis does not appear plausible because our observed results fall on the extreme tail of our distribution. Suggesting that the jury selection is likely influenced by systematic bias.

##### What might explain this discrepancy is changes in the eligible jury pool. Meaning that the composition of the county's population may have shifted, leading to incorrect expected proportions. Another explanation is injustice in the removal of jurors through premptory challenges. We could ask if there are specific populations getting removed at higher rates than others. To investigate this further, we could replicate this analysis across other county's and obtain updated demographic breakdowns in addition to further investigating dismissal patters on peremptory challenges.

```{r, echo=FALSE, results='hide'}

# Set the demographic breakdown of the country's eligible jury pool

expected_breakdown = c(Group_1 = 0.30, Group_2 = 0.25, Group_3 = 0.20, Group_4 = 0.15, Group_5 = 0.10)
observed_counts =  c(Group_1 = 85, Group_2 = 56, Group_3 = 59, Group_4 = 27, Group_5 = 13)
sum(observed_counts)

tibble(observed = observed_counts, expected = expected_breakdown*240)

# "multinomial sampling" equals sampling from a named set of categories
num_jurors = 240  # how many jurors per jury?
simulated_counts = rmultinom(1, num_jurors, expected_breakdown)


# compare "actual" with expected counts
simulated_counts - num_jurors*expected_breakdown


# Define a function to calculate the chi-squared statistic
chi_squared_statistic = function(observed, expected) {
  sum((observed - expected)^2 / expected)
}

chi2 = chi_squared_statistic(simulated_counts, num_jurors*expected_breakdown)
chi2

# Let's repeat this:
num_simulations = 10000
chi2_sim = do(num_simulations)*{
  simulated_counts = rmultinom(1, num_jurors, expected_breakdown)
  this_chi2 = chi_squared_statistic(simulated_counts, num_jurors*expected_breakdown)
  c(chi2 = this_chi2) # return a vector with names and values
}

ggplot(chi2_sim) + 
  geom_histogram(aes(x=chi2), binwidth = 1)


# Calculate X^2 based on the observed group counts for empaneled jurors
my_chi2 = chi_squared_statistic(observed_counts, num_jurors*expected_breakdown)
my_chi2

# p value?

chi2_sim %>%
  summarize(count(chi2 >=my_chi2) / n())


```


#### **Problem 4 - LLM Watermarking**
##### **Part A: The Null or Reference Distribution**

```{r, echo=FALSE, results='hide'}

#1 Read the sentences

original_sentences <- readLines("brown_sentences.txt")

#2 Preprocess the text

# Function to reprocess text
reprocess_text = function(text) {
  alphabet = paste0(LETTERS, collapse = "")
  
  # Remove non-letters and convert to uppercase
  clean_text = gsub("[^A-Za-z] ", "", text)
  clean_text = toupper(clean_text)
  
  return(clean_text)
}

#3 Reprocess all sentences from brown

new_sentences = reprocess_text(original_sentences)

# Calculate letter count, compare with expected count, and compute chi-squared statistic
calculate_chi_squared = function(sentence, freq_table) {
  
  # Ensure letter frequencies are normalized and sum to 1
  freq_table$Probability = freq_table$Probability / sum(freq_table$Probability)
  
  # Remove non-letters and convert to uppercase (call pre-defined function above)
  clean_sentence = reprocess_text(sentence)
  
  # Count the occurrences of each letter in the sentence
  observed_counts = table(factor(strsplit(clean_sentence, "")[[1]], levels = letter_frequencies$Letter))
  
  # Calculate expected counts
  total_letters = sum(observed_counts)
  expected_counts = total_letters * freq_table$Probability
  
  # Chi-squared statistic
  chi_squared_stat = sum((observed_counts - expected_counts)^2 / expected_counts)
  
  return(chi_squared_stat)
}

#4 Calculate the chi-square for EACH sentence. Use sapply for this. 

chi_squared_values = sapply(new_sentences, function(sentence){
  calculate_chi_squared(sentence, letter_frequencies)
})

#5 Compile the distribution 

plot(chi_squared_values, log = 'y')
```

##### **Part B: Checking for a Watermark**
##### The sentence that was produced by the LLM was sentence #6. "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland." We know that this sentence was produced by an LLM becasue out of the 10 sentences it had the lowest p-value: 0.009. Meaning that if the null hypthesis were true, there would be only a 0.9% probability of observing a standard letter frequency on this sentence. Thus, it is given that the low p-value for this sentence rejects the null hypthesis. 
```{r, echo=FALSE}

sentences_part_b <-c(
  "She opened the book and started to read the first chapter, eagerly anticipating what might come next.",
  "Despite the heavy rain, they decided to go for a long walk in the park, crossing the main avenue by the fountain in the center.",
  "The museum’s new exhibit features ancient artifacts from various civilizations around the world.",
  "He carefully examined the document, looking for any clues that might help solve the mystery.",
  "The students gathered in the auditorium to listen to the guest speaker’s inspiring lecture.",
  "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland.",
  "The chef demonstrated how to prepare a delicious meal using only locally sourced ingredients, focusing mainly on some excellent dinner recipes from Spain.",
  "They watched the sunset from the hilltop, marveling at the beautiful array of colors in the sky.",
  "The committee reviewed the proposal and provided many points of useful feedback to improve the project’s effectiveness.",
  "Despite the challenges faced during the project, the team worked tirelessly to ensure its successful completion, resulting in a product that exceeded everyone’s expectations."
)

# First let's calculate the chi-square statistic for each given sentence

chi_squared_values_part_b = sapply(sentences_part_b, function(sentence){
  calculate_chi_squared(sentence, letter_frequencies)
})

# p value?
p_values_part_b <- sapply(chi_squared_values_part_b, function(observed_chi_squared){
  p_value = sum(chi_squared_values >= observed_chi_squared) / length(chi_squared_values)
  return(round(p_value,3))
})

p_values_table <- data.frame(
  p_value = p_values_part_b)

kable(p_values_table)

#which.min(p_values_table$p_value)
```
